{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train long\n",
    "from mine import *\n",
    "from long import *\n",
    "from helper import *\n",
    "from utils import *\n",
    "\n",
    "start = 1550\n",
    "end = 1750\n",
    "\n",
    "env = Enviroment(stocks = ['AAPL'],start = start, end = end, lookback = 10, variance = 0.1,cluster = c)\n",
    "\n",
    "long_agent = DQN_Agent_Adapted(path_to_save,gamma,max_mem_size,C,batch_size,epsilon, eps_min, eps_dec, num_layers,hidden_size,added_size,fc1_dims,fc2_dims,lr,wd,lookback,x2_size,scheduler_max_value,scheduler_initial,mc)\n",
    "long_agent.load_weights(name = '1500_1700')\n",
    "long_agent.load_buffer()\n",
    "\n",
    "reward_running_count_finder = long\n",
    "long_agent.directory = r\"D:\\\\Project\\\\src\\\\Test_Policies\\\\long\\\\\"\n",
    "circle = 27\n",
    "\n",
    "while end <= 2870:\n",
    "    \n",
    "    # train kmeans\n",
    "    c.train(start,end)\n",
    "    \n",
    "    for it in range(itterations):\n",
    "        stock = np.random.choice(stocks)\n",
    "        env.change_and_reset(stocks = [stock])\n",
    "\n",
    "        running_count = 0\n",
    "        list_of_buy_days = []\n",
    "\n",
    "        treward = 0\n",
    "        for day in range(env.start,env.end):\n",
    "\n",
    "            done = day == env.end-1\n",
    "\n",
    "            x1 = env.feature_matrix(stock)\n",
    "            x2 = [running_count] + env.GOAT_features(stock)\n",
    "           \n",
    "            long_agent.Q_eval.eval()\n",
    "            with T.no_grad():\n",
    "                action = long_agent.choose_action(x1,x2)\n",
    "            long_agent.Q_eval.train()\n",
    "            \n",
    "            reward,running_count,list_of_buy_days = reward_running_count_finder(env,action,running_count,list_of_buy_days)\n",
    "            if done and running_count !=0:\n",
    "                done_reward,running_count,list_of_buy_days = reward_running_count_finder(env,0,running_count,list_of_buy_days)\n",
    "                reward = reward + done_reward\n",
    "                \n",
    "            treward += reward\n",
    "            x1_next = env.feature_matrix(stock)\n",
    "            x2_next = [running_count] + env.GOAT_features(stock)\n",
    "            \n",
    "            long_agent.store_transition(x1,x2,action,reward,x1_next,x2_next,done,circle)\n",
    "            long_agent.learn()\n",
    "            \n",
    "        env.reset()\n",
    "        #print(it)\n",
    "        \n",
    "    long_agent.save_weights(name = f'{start}_{end}')\n",
    "    \n",
    "    start += 50\n",
    "    end += 50\n",
    "    circle += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mine import *\n",
    "\n",
    "# train short\n",
    "from short import *\n",
    "from helper import *\n",
    "\n",
    "long_agent = DQN_Agent_Adapted(path_to_save,gamma,max_mem_size,C,batch_size,epsilon, eps_min, eps_dec, num_layers,hidden_size,added_size,fc1_dims,fc2_dims,lr,wd,lookback,x2_size,scheduler_max_value,scheduler_initial,mc)\n",
    "long_agent.load_weights(name = '1500_1700')\n",
    "long_agent.load_buffer()\n",
    "\n",
    "reward_running_count_finder = short\n",
    "long_agent.directory = r\"D:\\\\Project\\\\src\\\\Test_Policies\\\\short\\\\\"\n",
    "circle = 27\n",
    "\n",
    "while end <= 2870:\n",
    "    \n",
    "    # train kmeans\n",
    "    c.train(start,end)\n",
    "    \n",
    "    for it in range(itterations):\n",
    "        stock = np.random.choice(stocks)\n",
    "        env.change_and_reset(stocks = [stock])\n",
    "\n",
    "        running_count = 0\n",
    "        list_of_buy_days = []\n",
    "\n",
    "        treward = 0\n",
    "        for day in range(env.start,env.end):\n",
    "\n",
    "            done = day == env.end-1\n",
    "\n",
    "            x1 = env.feature_matrix(stock)\n",
    "            x2 = [running_count] + env.GOAT_features(stock)\n",
    "           \n",
    "            long_agent.Q_eval.eval()\n",
    "            with T.no_grad():\n",
    "                action = long_agent.choose_action(x1,x2)\n",
    "            long_agent.Q_eval.train()\n",
    "            \n",
    "            reward,running_count,list_of_buy_days = reward_running_count_finder(env,action,running_count,list_of_buy_days)\n",
    "            if done and running_count !=0:\n",
    "                done_reward,running_count,list_of_buy_days = reward_running_count_finder(env,0,running_count,list_of_buy_days)\n",
    "                reward = reward + done_reward\n",
    "                \n",
    "            treward += reward\n",
    "            x1_next = env.feature_matrix(stock)\n",
    "            x2_next = [running_count] + env.GOAT_features(stock)\n",
    "            \n",
    "            long_agent.store_transition(x1,x2,action,reward,x1_next,x2_next,done,circle)\n",
    "            long_agent.learn()\n",
    "            \n",
    "        env.reset()\n",
    "        #print(it)\n",
    "        \n",
    "    long_agent.save_weights(name = f'{start}_{end}')\n",
    "    \n",
    "    start += 50\n",
    "    end += 50\n",
    "    circle += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started_training\n",
      "D:\\Project\\src\\Weights\\SSA\\long_final\n"
     ]
    }
   ],
   "source": [
    "from mine import *\n",
    "from long import *\n",
    "\n",
    "# find policy long\n",
    "print(path_to_save)\n",
    "path = 'D:\\\\Project\\\\src\\\\Test_Policies\\\\final_policy\\\\'\n",
    "\n",
    "start = 1750\n",
    "end = 1800\n",
    "\n",
    "file_start = start -200\n",
    "file_end = start\n",
    "\n",
    "agent = DQN_Agent_Adapted(path_to_save,gamma,max_mem_size,C,batch_size,epsilon, eps_min, eps_dec, num_layers,hidden_size,added_size,fc1_dims,fc2_dims,lr,wd,lookback,x2_size,scheduler_max_value,scheduler_initial,mc)\n",
    "agent.directory = \"D:\\\\Project\\\\src\\\\Test_Policies\\\\long\\\\\"\n",
    "policy = np.zeros((1100,29))\n",
    "\n",
    "while end <= 2850:\n",
    "\n",
    "    c.train(file_start,file_end)\n",
    "\n",
    "    env = Enviroment(start = start, end = end, variance = 0, lookback = lookback,cluster = c,cash = 1e6,p_finder = shares)\n",
    "    agent.load_weights(name = f'{file_start}_{file_end}')\n",
    "\n",
    "    running_counts = [0 for _ in range(len(stocks))]\n",
    "    for day in range(start,end):\n",
    "        day_policy = [0 for _ in range(len(stocks))]\n",
    "\n",
    "        for number,stock in enumerate(stocks):\n",
    "            running_count = running_counts[number]\n",
    "            x1 = env.feature_matrix(stock)\n",
    "            x2 = [running_count] + env.GOAT_features(stock)\n",
    "\n",
    "            agent.Q_eval.eval()\n",
    "            with T.no_grad():\n",
    "                action = agent.choose_action(x1,x2)\n",
    "            agent.Q_eval.train()\n",
    "\n",
    "            if action == 2: running_counts[number] = running_counts[number] + 1\n",
    "            if action == 0: running_counts[number] = 0\n",
    "\n",
    "            day_policy[number] = action\n",
    "\n",
    "        policy[day - 1750,:] = day_policy\n",
    "        env.step(running_counts)\n",
    "\n",
    "    start += switch\n",
    "    end += switch\n",
    "    \n",
    "np.save(path + f'long.npy',policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started_training\n"
     ]
    }
   ],
   "source": [
    "from mine import *\n",
    "from short import *\n",
    "\n",
    "# find policy short\n",
    "\n",
    "path = 'D:\\\\Project\\\\src\\\\Test_Policies\\\\final_policy\\\\'\n",
    "\n",
    "start = 1750\n",
    "end = 1800\n",
    "\n",
    "file_start = start -200\n",
    "file_end = start\n",
    "\n",
    "agent = DQN_Agent_Adapted(path_to_save,gamma,max_mem_size,C,batch_size,epsilon, eps_min, eps_dec, num_layers,hidden_size,added_size,fc1_dims,fc2_dims,lr,wd,lookback,x2_size,scheduler_max_value,scheduler_initial,mc)\n",
    "agent.directory = \"D:\\\\Project\\\\src\\\\Test_Policies\\\\short\\\\\"\n",
    "policy = np.zeros((1100,29))\n",
    "\n",
    "while end <= 2850:\n",
    "\n",
    "    c.train(file_start,file_end)\n",
    "\n",
    "    env = Enviroment(start = start, end = end, variance = 0, lookback = lookback,cluster = c,cash = 1e6,p_finder = shares)\n",
    "    agent.load_weights(name = f'{file_start}_{file_end}')\n",
    "\n",
    "    running_counts = [0 for _ in range(len(stocks))]\n",
    "    for day in range(start,end):\n",
    "        day_policy = [0 for _ in range(len(stocks))]\n",
    "\n",
    "        for number,stock in enumerate(stocks):\n",
    "            running_count = running_counts[number]\n",
    "            x1 = env.feature_matrix(stock)\n",
    "            x2 = [running_count] + env.GOAT_features(stock)\n",
    "\n",
    "            agent.Q_eval.eval()\n",
    "            with T.no_grad():\n",
    "                action = agent.choose_action(x1,x2)\n",
    "            agent.Q_eval.train()\n",
    "\n",
    "            if action == 2: running_counts[number] = running_counts[number] + 1\n",
    "            if action == 0: running_counts[number] = 0\n",
    "\n",
    "            day_policy[number] = action\n",
    "\n",
    "        policy[day - 1750,:] = day_policy\n",
    "        env.step([-i for i in running_counts])\n",
    "\n",
    "    start += switch\n",
    "    end += switch\n",
    "    \n",
    "np.save(path + f'short.npy',policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
